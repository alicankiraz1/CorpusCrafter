"""
CorpusCrafter: PDF to AI Dataset Generator
-----------------------------------------
Bu modÃ¼l, PDF'ten CSV veri seti oluÅŸturucu iÃ§in ana iÅŸlevselliÄŸi saÄŸlar.
Metin Ã§Ä±karma, Ã¶n iÅŸleme, bÃ¶lÃ¼mleme ve soru oluÅŸturma iÅŸlemlerini iÃ§erir.
"""

import os
import sys
import time
import logging
import unicodedata
import re
import uuid
import argparse
from typing import List, Dict, Tuple, Optional, Any, Union, Callable
from pathlib import Path
import json
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm
import importlib.metadata

# Gerekli kÃ¼tÃ¼phaneler
import pandas as pd
import PyPDF2
from dotenv import load_dotenv
import openai
from langchain.text_splitter import (
    RecursiveCharacterTextSplitter,
    SentenceTransformersTokenTextSplitter,
    TokenTextSplitter
)
from langchain.schema import Document

# Ä°steÄŸe baÄŸlÄ± baÄŸÄ±mlÄ±lÄ±klarÄ± kontrol et
try:
    import langdetect
    LANGDETECT_AVAILABLE = True
except ImportError:
    LANGDETECT_AVAILABLE = False

try:
    import spacy
    SPACY_AVAILABLE = True
except ImportError:
    SPACY_AVAILABLE = False

# Versiyon bilgisi
__version__ = "0.2.1"

# Ortam deÄŸiÅŸkenlerini yÃ¼kle
load_dotenv()

# GÃ¼nlÃ¼k kaydÄ±nÄ± yapÄ±landÄ±r
def configure_logging(log_file="pdf_to_csv.log", log_level=logging.INFO):
    """
    GÃ¼nlÃ¼k kaydÄ±nÄ± yapÄ±landÄ±r.
    
    Args:
        log_file: GÃ¼nlÃ¼k dosyasÄ±nÄ±n adÄ±
        log_level: GÃ¼nlÃ¼k seviyesi
    """
    logging.basicConfig(
        level=log_level,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_file),
            logging.StreamHandler()
        ]
    )
    return logging.getLogger(__name__)

# VarsayÄ±lan gÃ¼nlÃ¼k kaydÄ±nÄ± yapÄ±landÄ±r
logger = configure_logging()

# OpenAI API anahtarÄ±nÄ± kontrol et
def check_openai_api_key():
    """
    OpenAI API anahtarÄ±nÄ± kontrol et ve istemciyi yapÄ±landÄ±r.
    
    Returns:
        OpenAI istemcisi
    """
    api_key = os.environ.get("OPENAI_API_KEY")
    if not api_key:
        logger.error("OPENAI_API_KEY ortam deÄŸiÅŸkeni bulunamadÄ±!")
        print("Hata: OPENAI_API_KEY ortam deÄŸiÅŸkeni ayarlanmamÄ±ÅŸ. LÃ¼tfen API anahtarÄ±nÄ±zÄ± ayarlayÄ±n.")
        print("Ã–rnek: export OPENAI_API_KEY=your_api_key_here")
        sys.exit(1)
    
    return openai.OpenAI(api_key=api_key)

# Test modu iÃ§in sahte OpenAI istemcisi
class MockOpenAI:
    """
    Test iÃ§in sahte OpenAI istemcisi.
    """
    class ChatCompletion:
        class Create:
            def __init__(self, choices):
                self.choices = choices
        
        @staticmethod
        def create(**kwargs):
            mock_response = MockOpenAI.ChatCompletion.Create(
                choices=[
                    type('obj', (object,), {
                        'message': type('obj', (object,), {
                            'content': "Bu bir test sorusudur?"
                        })
                    })
                ]
            )
            return mock_response

    chat = type('obj', (object,), {'completions': ChatCompletion})

# OpenAI istemcisini yapÄ±landÄ±r
client = None
TEST_MODE = os.environ.get("CORPUSCRAFTER_TEST_MODE", "false").lower() == "true"

if TEST_MODE:
    logger.info("Test modu etkin, sahte OpenAI istemcisi kullanÄ±lÄ±yor")
    client = MockOpenAI()
else:
    client = check_openai_api_key()

# KullanÄ±labilir OpenAI modelleri
AVAILABLE_MODELS = {
    "gpt-3.5-turbo": {
        "name": "GPT-3.5 Turbo",
        "description": "Genel amaÃ§lÄ± kullanÄ±m iÃ§in hÄ±zlÄ± ve ekonomik model.",
        "max_tokens": 4096,
        "default_params": {"temperature": 0.7, "top_p": 0.9, "max_tokens": 150}
    },
    "gpt-4o-mini": {
        "name": "GPT-4o Mini",
        "description": "GPT-4o'nun daha kÃ¼Ã§Ã¼k ve daha hÄ±zlÄ± versiyonu.",
        "max_tokens": 8192,
        "default_params": {"temperature": 0.7, "top_p": 0.9, "max_tokens": 150}
    },
    "gpt-4o": {
        "name": "GPT-4o",
        "description": "YÃ¼ksek kaliteli yanÄ±tlar iÃ§in ideal, en geliÅŸmiÅŸ Ã§ok modlu model.",
        "max_tokens": 8192,
        "default_params": {"temperature": 0.7, "top_p": 0.9, "max_tokens": 150}
    },
    "gpt-4-turbo": {
        "name": "GPT-4 Turbo",
        "description": "GPT-4'Ã¼n daha hÄ±zlÄ± ve daha ekonomik versiyonu.",
        "max_tokens": 4096,
        "default_params": {"temperature": 0.7, "top_p": 0.9, "max_tokens": 150}
    }
}

# VarsayÄ±lan model
DEFAULT_MODEL = "gpt-4o-mini"

# Metin bÃ¶lme algoritmalarÄ±
TEXT_SPLITTER_TYPES = {
    "recursive": {
        "name": "Recursive Character Splitter",
        "description": "Metni belirli ayÄ±rÄ±cÄ±lara gÃ¶re Ã¶zyinelemeli olarak bÃ¶ler.",
        "class": RecursiveCharacterTextSplitter
    },
    "sentence_transformers": {
        "name": "Sentence Transformers Token Splitter",
        "description": "Metni cÃ¼mle transformers modelini kullanarak tokenlara bÃ¶ler.",
        "class": SentenceTransformersTokenTextSplitter
    },
    "token": {
        "name": "Token Splitter",
        "description": "Metni tokenlara gÃ¶re bÃ¶ler.",
        "class": TokenTextSplitter
    }
}

# VarsayÄ±lan metin bÃ¶lÃ¼cÃ¼
DEFAULT_TEXT_SPLITTER = "recursive"

# Desteklenen diller ve Ã¶zellikleri
SUPPORTED_LANGUAGES = {
    "en": {
        "name": "English",
        "heading_patterns": [
            r'^[A-Z0-9]',       # BÃ¼yÃ¼k harf veya rakamla baÅŸlayan
            r'^[IVX]+\.',       # Roma rakamlarÄ±yla baÅŸlayan (I., II., vb.)
            r'^\d+\.\s',        # NumaralandÄ±rÄ±lmÄ±ÅŸ baÅŸlÄ±k (1. Introduction, vb.)
            r'^[A-Za-z]+\s\d+', # "Chapter 1", "Section 2" vb.
            r'^[A-Z\s]+$'       # Tamamen bÃ¼yÃ¼k harflerden oluÅŸan
        ],
        "question_starters": [
            'what ', 'why ', 'how ', 'which ', 'who ', 'whom ', 'whose ',
            'where ', 'when ', 'can ', 'could ', 'would ', 'should ', 'is ',
            'are ', 'do ', 'does ', 'did ', 'have ', 'has ', 'had '
        ]
    },
    "tr": {
        "name": "Turkish",
        "heading_patterns": [
            r'^[A-Z0-9]',       # BÃ¼yÃ¼k harfle veya rakamla baÅŸlayan
            r'^[IVX]+\.',       # Roma rakamlarÄ± ile baÅŸlayan (I., II., vb.)
            r'^\d+\.\s',        # NumaralandÄ±rÄ±lmÄ±ÅŸ baÅŸlÄ±k (1. GiriÅŸ, vb.)
            r'^[A-Za-z]+\s\d+', # "BÃ¶lÃ¼m 1", "KÄ±sÄ±m 2" gibi
            r'^[A-Z\s]+$',      # Tamamen bÃ¼yÃ¼k harflerden oluÅŸan
            r'^[A-Za-z]+\s\d+:' # "BÃ¶lÃ¼m 1:" gibi
        ],
        "question_starters": [
            'ne ', 'neden ', 'niÃ§in ', 'niye ', 'nasÄ±l ', 'hangi ', 'kim ', 'kime ', 'kimi ',
            'nerede ', 'ne zaman ', 'kaÃ§ ', 'kaÃ§ta ', 'nereye ', 'nereden ', 'mi ', 'mÄ± ',
            'mu ', 'mÃ¼ '
        ]
    },
    # Ek diller buraya eklenebilir
}

# VarsayÄ±lan dil
DEFAULT_LANGUAGE = "auto"

class PDFProcessor:
    """PDF dosyalarÄ±nÄ± iÅŸleme ve metin Ã§Ä±karma sÄ±nÄ±fÄ±."""
    
    def __init__(self, input_dir: str = "input", language: str = DEFAULT_LANGUAGE):
        """
        PDF iÅŸleyiciyi baÅŸlat.
        
        Args:
            input_dir: PDF dosyalarÄ±nÄ± iÃ§eren dizin
            language: Belge dili (otomatik algÄ±lama iÃ§in "auto")
        """
        self.input_dir = input_dir
        self.language = language
        
    def get_pdf_path(self, specific_file: Optional[str] = None) -> str:
        """
        GiriÅŸ dizininden bir PDF dosyasÄ± seÃ§.
        
        Args:
            specific_file: Ä°ÅŸlenecek belirli bir dosya adÄ± (isteÄŸe baÄŸlÄ±)
            
        Returns:
            SeÃ§ilen PDF'nin tam yolu
            
        Raises:
            FileNotFoundError: Belirtilen dosya veya dizinde PDF bulunamazsa
        """
        if specific_file:
            pdf_path = os.path.join(self.input_dir, specific_file)
            if not os.path.exists(pdf_path):
                error_msg = f"Belirtilen dosya bulunamadÄ±: {pdf_path}"
                logger.error(error_msg)
                raise FileNotFoundError(error_msg)
            return pdf_path
        
        # Dizindeki ilk PDF dosyasÄ±nÄ± bul
        try:
            pdf_files = [f for f in os.listdir(self.input_dir) if f.lower().endswith('.pdf')]
        except FileNotFoundError:
            error_msg = f"GiriÅŸ dizini bulunamadÄ±: {self.input_dir}"
            logger.error(error_msg)
            raise FileNotFoundError(error_msg)
        
        if not pdf_files:
            error_msg = f"{self.input_dir} dizininde PDF dosyasÄ± bulunamadÄ±!"
            logger.error(error_msg)
            raise FileNotFoundError(error_msg)
        
        # Ä°lk PDF dosyasÄ±nÄ± seÃ§
        selected_pdf = pdf_files[0]
        pdf_path = os.path.join(self.input_dir, selected_pdf)
        logger.info(f"SeÃ§ilen PDF: {pdf_path}")
        
        return pdf_path
    
    def extract_text_from_pdf(self, pdf_path: str) -> Tuple[str, Dict[str, Any]]:
        """
        PDF dosyasÄ±ndan metin Ã§Ä±kar ve gereksiz kÄ±sÄ±mlarÄ± filtrele.
        
        Args:
            pdf_path: PDF dosyasÄ±nÄ±n tam yolu
            
        Returns:
            Tuple[str, Dict[str, Any]]: Ã‡Ä±karÄ±lan metin ve istatistikler
            
        Raises:
            Exception: PDF okuma hatasÄ± durumunda
        """
        logger.info(f"PDF dosyasÄ± okunuyor: {pdf_path}")
        
        stats = {
            "total_pages": 0,
            "skipped_pages": 0,
            "empty_pages": 0,
            "image_pages": 0
        }
        
        extracted_text = ""
        
        try:
            with open(pdf_path, 'rb') as file:
                reader = PyPDF2.PdfReader(file)
                stats["total_pages"] = len(reader.pages)
                
                # Ä°lk birkaÃ§ ve son birkaÃ§ sayfayÄ± atla (kapak, Ã¶nsÃ¶z, dizin vb.)
                start_page = min(1, stats["total_pages"] - 1)  # Kapak sayfasÄ±nÄ± atla
                end_page = max(start_page, stats["total_pages"] - 2)  # Son sayfalarÄ± atla
                
                for i in range(start_page, end_page):
                    page = reader.pages[i]
                    page_text = page.extract_text()
                    
                    # BoÅŸ sayfalarÄ± atla
                    if not page_text or page_text.isspace():
                        stats["empty_pages"] += 1
                        stats["skipped_pages"] += 1
                        continue
                    
                    # Ã‡oÄŸunlukla gÃ¶rÃ¼ntÃ¼ iÃ§eren sayfalarÄ± algÄ±la ve atla
                    # (Ã‡ok az metin iÃ§eren sayfalar genellikle gÃ¶rÃ¼ntÃ¼ aÄŸÄ±rlÄ±klÄ±dÄ±r)
                    if len(page_text.split()) < 20:
                        stats["image_pages"] += 1
                        stats["skipped_pages"] += 1
                        continue
                    
                    # DipnotlarÄ± ve gÃ¶rÃ¼ntÃ¼ baÅŸlÄ±klarÄ±nÄ± filtrele
                    # (Basit yaklaÅŸÄ±m: KÄ±sa satÄ±rlarÄ± ve rakamla baÅŸlayan satÄ±rlarÄ± atla)
                    filtered_lines = []
                    for line in page_text.split('\n'):
                        # Dipnot ve referans filtreleme
                        if re.match(r'^\d+\s', line) and len(line) < 100:
                            continue
                        # Ã‡ok kÄ±sa satÄ±rlarÄ± atla (muhtemelen baÅŸlÄ±k veya baÅŸlÄ±k)
                        if len(line.strip()) < 5:
                            continue
                        filtered_lines.append(line)
                    
                    extracted_text += '\n'.join(filtered_lines) + '\n\n'
        
        except Exception as e:
            logger.error(f"PDF okuma hatasÄ±: {str(e)}")
            raise
        
        logger.info(f"PDF metin Ã§Ä±karma tamamlandÄ±. Toplam {stats['total_pages']} sayfa, "
                    f"{stats['skipped_pages']} sayfa atlandÄ±.")
        
        return extracted_text, stats
    
    def detect_language(self, text: str) -> str:
        """
        Metnin dilini algÄ±la.
        
        Args:
            text: Analiz edilecek metin
            
        Returns:
            ISO dil kodu (Ã¶rn. 'en', 'tr')
        """
        if self.language != "auto":
            return self.language
        
        if not LANGDETECT_AVAILABLE:
            logger.warning("langdetect yÃ¼klÃ¼ deÄŸil. VarsayÄ±lan olarak Ä°ngilizce kullanÄ±lÄ±yor.")
            return "en"
        
        try:
            # Daha hÄ±zlÄ± algÄ±lama iÃ§in sadece ilk 1000 karakteri kullan
            sample = text[:1000]
            lang = langdetect.detect(sample)
            logger.info(f"AlgÄ±lanan dil: {lang}")
            
            # Desteklenen dillere eÅŸle veya varsayÄ±lan olarak Ä°ngilizce kullan
            if lang in SUPPORTED_LANGUAGES:
                return lang
            else:
                logger.warning(f"AlgÄ±lanan dil {lang} tam olarak desteklenmiyor. Ä°ngilizce kalÄ±plar kullanÄ±lÄ±yor.")
                return "en"
        except Exception as e:
            logger.warning(f"Dil algÄ±lama baÅŸarÄ±sÄ±z: {str(e)}. VarsayÄ±lan olarak Ä°ngilizce kullanÄ±lÄ±yor.")
            return "en"


class TextProcessor:
    """Metin Ã¶n iÅŸleme ve temizleme sÄ±nÄ±fÄ±."""
    
    def __init__(self, language: str = "en"):
        """
        Metin iÅŸleyiciyi baÅŸlat.
        
        Args:
            language: ISO dil kodu (Ã¶rn. 'en', 'tr')
        """
        self.language = language
        
        # Dile Ã¶zgÃ¼ kalÄ±plarÄ± al
        if language in SUPPORTED_LANGUAGES:
            self.lang_config = SUPPORTED_LANGUAGES[language]
        else:
            logger.warning(f"Dil {language} desteklenmiyor. Ä°ngilizce kalÄ±plar kullanÄ±lÄ±yor.")
            self.language = "en"
            self.lang_config = SUPPORTED_LANGUAGES["en"]
        
        # spaCy'yi baÅŸlat (mevcutsa)
        self.nlp = None
        if SPACY_AVAILABLE:
            try:
                if language == "en":
                    self.nlp = spacy.load("en_core_web_sm")
                elif language == "tr":
                    self.nlp = spacy.load("tr_core_news_sm")
                # GerektiÄŸinde daha fazla dil ekle
            except OSError:
                logger.warning(f"spaCy modeli {language} iÃ§in bulunamadÄ±. Model indirme talimatlarÄ±:")
                if language == "en":
                    logger.warning("python -m spacy download en_core_web_sm")
                elif language == "tr":
                    logger.warning("python -m spacy download tr_core_news_sm")
    
    def is_heading(self, line: str) -> bool:
        """
        Bir satÄ±rÄ±n baÅŸlÄ±k olup olmadÄ±ÄŸÄ±nÄ± kontrol et.
        
        Args:
            line: Kontrol edilecek satÄ±r
            
        Returns:
            bool: SatÄ±r bir baÅŸlÄ±ksa True, deÄŸilse False
        """
        line = line.strip()
        
        if not line:
            return False
        
        # Ã‡ok uzun satÄ±rlar baÅŸlÄ±k deÄŸildir
        if len(line) > 100:
            return False
        
        # Nokta veya soru iÅŸareti iÃ§eren satÄ±rlar genellikle baÅŸlÄ±k deÄŸildir
        if '.' in line[:-1] or '?' in line:
            return False
        
        # Dile Ã¶zgÃ¼ baÅŸlÄ±k kalÄ±plarÄ±na gÃ¶re kontrol et
        for pattern in self.lang_config["heading_patterns"]:
            if re.match(pattern, line):
                return True
        
        return False
    
    def is_question(self, line: str) -> bool:
        """
        Bir satÄ±rÄ±n soru olup olmadÄ±ÄŸÄ±nÄ± kontrol et.
        
        Args:
            line: Kontrol edilecek satÄ±r
            
        Returns:
            bool: SatÄ±r bir soruysa True, deÄŸilse False
        """
        line = line.strip()
        
        if not line:
            return False
        
        # Soru iÅŸaretiyle biten satÄ±rlar
        if line.endswith('?'):
            return True
        
        # Soru kelimeleriyle baÅŸlayan satÄ±rlar
        lower_line = line.lower()
        for starter in self.lang_config["question_starters"]:
            if lower_line.startswith(starter):
                return True
        
        return False
    
    def preprocess_text_advanced(self, text: str) -> str:
        """
        GeliÅŸmiÅŸ metin Ã¶n iÅŸleme: BaÅŸlÄ±klarÄ± ve sorularÄ± atla,
        baÅŸlÄ±klar altÄ±ndaki iÃ§eriÄŸi tek paragrafta birleÅŸtir.
        
        Args:
            text: Ä°ÅŸlenecek ham metin
            
        Returns:
            Ä°ÅŸlenmiÅŸ metin
        """
        logger.info("GeliÅŸmiÅŸ metin Ã¶n iÅŸleme baÅŸlatÄ±lÄ±yor")
        
        # Unicode normalizasyonu (NFKC)
        text = unicodedata.normalize('NFKC', text)
        
        # SatÄ±rlara bÃ¶l
        lines = text.split('\n')
        
        # Ä°ÅŸlenmiÅŸ metin iÃ§in
        processed_paragraphs = []
        current_paragraph = []
        
        # BaÅŸlÄ±klar altÄ±ndaki iÃ§eriÄŸi toplamak iÃ§in
        in_content_section = False
        
        for line in lines:
            line = line.strip()
            
            # BoÅŸ satÄ±rlarÄ± atla
            if not line:
                # Bir paragraf biriktirilmiÅŸse, kaydet ve yeni bir paragrafa baÅŸla
                if current_paragraph:
                    processed_paragraphs.append(' '.join(current_paragraph))
                    current_paragraph = []
                in_content_section = False
                continue
            
            # BaÅŸlÄ±k kontrolÃ¼
            if self.is_heading(line):
                # Bir paragraf biriktirilmiÅŸse, kaydet
                if current_paragraph:
                    processed_paragraphs.append(' '.join(current_paragraph))
                    current_paragraph = []
                
                # BaÅŸlÄ±ÄŸÄ± atla, ancak iÃ§erik bÃ¶lÃ¼mÃ¼nde olduÄŸumuzu iÅŸaretle
                in_content_section = True
                continue
            
            # Soru kontrolÃ¼
            if self.is_question(line):
                # SorularÄ± atla
                continue
            
            # Ä°Ã§erik bÃ¶lÃ¼mÃ¼ndeyse ve geÃ§erli bir satÄ±rsa, paragrafa ekle
            if in_content_section:
                # Sondaki boÅŸluklarÄ± temizle
                line = line.strip()
                
                # Ã‡ok kÄ±sa satÄ±rlarÄ± atla (muhtemelen alt baÅŸlÄ±klar veya liste Ã¶ÄŸeleri)
                if len(line) < 20:
                    continue
                    
                current_paragraph.append(line)
        
        # Son paragrafÄ± ekle
        if current_paragraph:
            processed_paragraphs.append(' '.join(current_paragraph))
        
        # ParagraflarÄ± birleÅŸtir
        processed_text = '\n\n'.join(processed_paragraphs)
        
        # ArdÄ±ÅŸÄ±k boÅŸluklarÄ± tek bir boÅŸlukla deÄŸiÅŸtir
        processed_text = re.sub(r'\s+', ' ', processed_text)
        
        # ArdÄ±ÅŸÄ±k satÄ±r sonlarÄ±nÄ± temizle
        processed_text = re.sub(r'\n\s*\n', '\n\n', processed_text)
        
        logger.info("GeliÅŸmiÅŸ metin Ã¶n iÅŸleme tamamlandÄ±")
        return processed_text
    
    def preprocess_with_spacy(self, text: str) -> str:
        """
        spaCy kullanarak metin Ã¶n iÅŸleme.
        
        Args:
            text: Ä°ÅŸlenecek metin
            
        Returns:
            Ä°ÅŸlenmiÅŸ metin
            
        Raises:
            ValueError: spaCy yÃ¼klÃ¼ deÄŸilse veya model bulunamazsa
        """
        if not SPACY_AVAILABLE or self.nlp is None:
            raise ValueError(f"spaCy veya {self.language} dil modeli yÃ¼klÃ¼ deÄŸil")
        
        logger.info("spaCy tabanlÄ± metin Ã¶n iÅŸleme baÅŸlatÄ±lÄ±yor")
        
        # Unicode normalizasyonu
        text = unicodedata.normalize('NFKC', text)
        
        # spaCy ile iÅŸle
        doc = self.nlp(text)
        
        # BaÅŸlÄ±klarÄ± ve sorularÄ± tanÄ±mla
        headings = []
        questions = []
        
        for sent in doc.sents:
            sent_text = sent.text.strip()
            if self.is_heading(sent_text):
                headings.append(sent_text)
            elif self.is_question(sent_text):
                questions.append(sent_text)
        
        # BaÅŸlÄ±klarÄ± ve sorularÄ± filtrele
        processed_paragraphs = []
        current_paragraph = []
        
        for sent in doc.sents:
            sent_text = sent.text.strip()
            
            # BaÅŸlÄ±klarÄ± ve sorularÄ± atla
            if sent_text in headings or sent_text in questions:
                continue
            
            # Ã‡ok kÄ±sa cÃ¼mleleri atla
            if len(sent_text) < 20:
                continue
            
            # Bu yeni bir paragrafÄ±n baÅŸlangÄ±cÄ±ysa
            if sent.start_char > 0 and doc.text[sent.start_char-1] == '\n':
                if current_paragraph:
                    processed_paragraphs.append(' '.join(current_paragraph))
                    current_paragraph = []
            
            current_paragraph.append(sent_text)
        
        # Son paragrafÄ± ekle
        if current_paragraph:
            processed_paragraphs.append(' '.join(current_paragraph))
        
        # ParagraflarÄ± birleÅŸtir
        processed_text = '\n\n'.join(processed_paragraphs)
        
        logger.info("spaCy tabanlÄ± metin Ã¶n iÅŸleme tamamlandÄ±")
        return processed_text


class TextChunker:
    """Metni anlamlÄ± parÃ§alara bÃ¶lme sÄ±nÄ±fÄ±."""
    
    def __init__(self, splitter_type: str = DEFAULT_TEXT_SPLITTER,
                 chunk_size: int = 2000, chunk_overlap: int = 200,
                 custom_separators: Optional[List[str]] = None):
        """
        Metin bÃ¶lÃ¼cÃ¼yÃ¼ baÅŸlat.
        
        Args:
            splitter_type: KullanÄ±lacak metin bÃ¶lÃ¼cÃ¼ tÃ¼rÃ¼
            chunk_size: Her parÃ§anÄ±n karakter cinsinden boyutu
            chunk_overlap: ParÃ§alar arasÄ±ndaki Ã¶rtÃ¼ÅŸme (karakter cinsinden)
            custom_separators: Ã–zel ayÄ±rÄ±cÄ±lar listesi (isteÄŸe baÄŸlÄ±)
        """
        self.splitter_type = splitter_type
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.custom_separators = custom_separators
        
        # BÃ¶lÃ¼cÃ¼ tÃ¼rÃ¼nÃ¼ doÄŸrula
        if splitter_type not in TEXT_SPLITTER_TYPES:
            logger.warning(f"Belirtilen bÃ¶lÃ¼cÃ¼ tÃ¼rÃ¼ ({splitter_type}) bulunamadÄ±. VarsayÄ±lan kullanÄ±lÄ±yor.")
            self.splitter_type = DEFAULT_TEXT_SPLITTER
        
        self.splitter_class = TEXT_SPLITTER_TYPES[self.splitter_type]["class"]
    
    def chunk_text_semantic(self, text: str) -> List[Document]:
        """
        Metni anlamlÄ± parÃ§alara bÃ¶l, paragraf bÃ¼tÃ¼nlÃ¼ÄŸÃ¼nÃ¼ koru.
        
        Args:
            text: BÃ¶lÃ¼necek metin
            
        Returns:
            Document nesneleri listesi
        """
        logger.info(f"Anlamsal metin bÃ¶lÃ¼mleme baÅŸlatÄ±lÄ±yor (BÃ¶lÃ¼cÃ¼: {self.splitter_type}, "
                    f"Boyut: {self.chunk_size}, Ã–rtÃ¼ÅŸme: {self.chunk_overlap})")
        
        # BÃ¶lÃ¼cÃ¼ parametrelerini yapÄ±landÄ±r
        if self.splitter_type == "recursive":
            # VarsayÄ±lan ayÄ±rÄ±cÄ±lar
            separators = ["\n\n", "\n", ". ", " ", ""]
            
            # Ã–zel ayÄ±rÄ±cÄ±lar belirtilmiÅŸse kullan
            if self.custom_separators:
                separators = self.custom_separators + [""]
            
            text_splitter = self.splitter_class(
                chunk_size=self.chunk_size,
                chunk_overlap=self.chunk_overlap,
                length_function=len,
                separators=separators
            )
        elif self.splitter_type == "sentence_transformers":
            # Karakter sayÄ±sÄ± yerine token sayÄ±sÄ± iÃ§in ayarla
            token_chunk_size = max(1, self.chunk_size // 4)
            token_chunk_overlap = max(1, self.chunk_overlap // 4)
            
            text_splitter = self.splitter_class(
                chunk_size=token_chunk_size,
                chunk_overlap=token_chunk_overlap
            )
        else:  # token
            # Karakter sayÄ±sÄ± yerine token sayÄ±sÄ± iÃ§in ayarla
            token_chunk_size = max(1, self.chunk_size // 4)
            token_chunk_overlap = max(1, self.chunk_overlap // 4)
            
            text_splitter = self.splitter_class(
                chunk_size=token_chunk_size,
                chunk_overlap=token_chunk_overlap
            )
        
        # Metni parÃ§alara bÃ¶l ve her parÃ§a iÃ§in benzersiz ID oluÅŸtur
        chunks = text_splitter.create_documents([text])
        
        # Her parÃ§aya benzersiz ID ekle
        for chunk in chunks:
            chunk.metadata["chunk_id"] = str(uuid.uuid4())
        
        logger.info(f"Anlamsal metin bÃ¶lÃ¼mleme tamamlandÄ±. {len(chunks)} parÃ§a oluÅŸturuldu.")
        return chunks


class QuestionGenerator:
    """AI modelleri kullanarak metin parÃ§alarÄ±ndan soru oluÅŸturma sÄ±nÄ±fÄ±."""
    
    def __init__(self, model: str = DEFAULT_MODEL, system_prompt: Optional[str] = None):
        """
        Soru oluÅŸturucuyu baÅŸlat.
        
        Args:
            model: KullanÄ±lacak OpenAI modeli
            system_prompt: Ã–zel sistem mesajÄ± (isteÄŸe baÄŸlÄ±)
        """
        self.model = model
        self.system_prompt = system_prompt
        
        # Modeli doÄŸrula
        if model not in AVAILABLE_MODELS:
            logger.warning(f"Belirtilen model ({model}) bulunamadÄ±. VarsayÄ±lan kullanÄ±lÄ±yor.")
            self.model = DEFAULT_MODEL
        
        self.model_info = AVAILABLE_MODELS[self.model]
        self.model_params = self.model_info["default_params"].copy()
    
    def generate_question(self, chunk_text: str, language: str = "en",
                        retries: int = 3, backoff_factor: float = 2.0) -> str:
        """
        Bir metin parÃ§asÄ± iÃ§in OpenAI API kullanarak soru oluÅŸtur.
        
        Args:
            chunk_text: Soru oluÅŸturulacak metin parÃ§asÄ±
            language: Metnin dili
            retries: Yeniden deneme sayÄ±sÄ±
            backoff_factor: Yeniden denemeler iÃ§in geri Ã§ekilme faktÃ¶rÃ¼
            
        Returns:
            OluÅŸturulan soru
            
        Raises:
            Exception: API hatasÄ± durumunda
        """
        # Dile gÃ¶re prompt ayarla
        if language == "tr":
            user_prompt = f"""
            AÅŸaÄŸÄ±daki metni oku ve bu metin hakkÄ±nda en alakalÄ±, aÃ§Ä±k uÃ§lu tek bir soru oluÅŸtur.
            Soru, metindeki ana fikri veya Ã¶nemli bir kavramÄ± anlamaya yÃ¶nelik olmalÄ±dÄ±r.
            
            Metin:
            {chunk_text}
            
            Soru:
            """
        else:  # VarsayÄ±lan olarak Ä°ngilizce
            user_prompt = f"""
            Read the following text and create a single, relevant, open-ended question about it.
            The question should aim to understand the main idea or an important concept in the text.
            
            Text:
            {chunk_text}
            
            Question:
            """
        
        # Sistem mesajÄ±nÄ± ayarla
        if self.system_prompt:
            system_message = self.system_prompt
        else:
            if language == "tr":
                system_message = "Sen yardÄ±mcÄ± bir eÄŸitim asistanÄ±sÄ±n. Verilen metinler hakkÄ±nda dÃ¼ÅŸÃ¼ndÃ¼rÃ¼cÃ¼ ve anlamlÄ± sorular oluÅŸturursun."
            else:
                system_message = "You are a helpful educational assistant. You create thoughtful and meaningful questions about given texts."
        
        # Test modunda sahte yanÄ±t dÃ¶ndÃ¼r
        if TEST_MODE:
            if language == "tr":
                return "Bu metin hakkÄ±nda en Ã¶nemli kavram nedir?"
            else:
                return "What is the most important concept in this text?"
        
        # API Ã§aÄŸrÄ±sÄ± iÃ§in yeniden deneme mantÄ±ÄŸÄ±
        for attempt in range(retries):
            try:
                # Jitter ekleyerek geri Ã§ekilme sÃ¼resi hesapla
                if attempt > 0:
                    jitter = random.uniform(0.8, 1.2)
                    sleep_time = backoff_factor ** attempt * jitter
                    logger.info(f"{sleep_time:.2f} saniye bekleniyor...")
                    time.sleep(sleep_time)
                
                response = client.chat.completions.create(
                    model=self.model,
                    messages=[
                        {"role": "system", "content": system_message},
                        {"role": "user", "content": user_prompt}
                    ],
                    temperature=self.model_params["temperature"],
                    top_p=self.model_params["top_p"],
                    max_tokens=self.model_params["max_tokens"]
                )
                
                question = response.choices[0].message.content.strip()
                return question
                
            except Exception as e:
                error_msg = f"API hatasÄ± (deneme {attempt+1}/{retries}): {str(e)}"
                logger.warning(error_msg)
                
                if attempt < retries - 1:
                    continue
                else:
                    logger.error(f"Maksimum yeniden deneme sayÄ±sÄ±na ulaÅŸÄ±ldÄ±. Son hata: {str(e)}")
                    if language == "tr":
                        return "API hatasÄ± nedeniyle soru oluÅŸturulamadÄ±."
                    else:
                        return "Could not generate question due to API error."
        
        # Bu noktaya ulaÅŸÄ±lmamalÄ±, ancak gÃ¼venlik iÃ§in
        return "Question generation failed."


class DatasetCreator:
    """Metin parÃ§alarÄ± ve sorulardan CSV veri setleri oluÅŸturma sÄ±nÄ±fÄ±."""
    
    def __init__(self, output_dir: str = "output"):
        """
        Veri seti oluÅŸturucuyu baÅŸlat.
        
        Args:
            output_dir: CSV dosyalarÄ± iÃ§in Ã§Ä±ktÄ± dizini
        """
        self.output_dir = output_dir
        
        # Ã‡Ä±ktÄ± dizini yoksa oluÅŸtur
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)
            logger.info(f"Ã‡Ä±ktÄ± dizini oluÅŸturuldu: {output_dir}")
    
    def create_csv_dataset(self, chunks: List[Document], 
                          question_generator: QuestionGenerator,
                          pdf_name: str, language: str = "en",
                          system_prompt: Optional[str] = None,
                          max_workers: int = 1) -> str:
        """
        Metin parÃ§alarÄ± ve oluÅŸturulan sorulardan bir CSV veri seti oluÅŸtur.
        
        Args:
            chunks: Document nesneleri listesi
            question_generator: QuestionGenerator Ã¶rneÄŸi
            pdf_name: PDF dosyasÄ±nÄ±n adÄ±
            language: Metnin dili
            system_prompt: Sistem sÃ¼tunu iÃ§in mesaj (isteÄŸe baÄŸlÄ±)
            max_workers: Paralel iÅŸleme iÃ§in maksimum iÅŸ parÃ§acÄ±ÄŸÄ± sayÄ±sÄ±
            
        Returns:
            OluÅŸturulan CSV dosyasÄ±nÄ±n yolu
        """
        logger.info(f"CSV veri seti oluÅŸturma baÅŸlatÄ±lÄ±yor (Model: {question_generator.model})")
        
        # Veri Ã§erÃ§evesi oluÅŸtur
        data = []
        
        # Soru oluÅŸturma fonksiyonu
        def generate_question_for_chunk(chunk_with_index):
            index, chunk = chunk_with_index
            chunk_id = chunk.metadata["chunk_id"]
            user_text = chunk.page_content
            
            try:
                # Soru oluÅŸtur
                question = question_generator.generate_question(user_text, language)
                
                return {
                    "chunk_id": chunk_id,
                    "system": system_prompt or "",
                    "user": user_text,
                    "assistant": question
                }
            except Exception as e:
                logger.error(f"ParÃ§a {index} iÃ§in soru oluÅŸturma hatasÄ±: {str(e)}")
                # Hata durumunda boÅŸ soru
                if language == "tr":
                    error_question = "Soru oluÅŸturma sÄ±rasÄ±nda bir hata oluÅŸtu."
                else:
                    error_question = "An error occurred during question generation."
                
                return {
                    "chunk_id": chunk_id,
                    "system": system_prompt or "",
                    "user": user_text,
                    "assistant": error_question
                }
        
        # Paralel iÅŸleme kullan (max_workers > 1 ise)
        if max_workers > 1 and not TEST_MODE:
            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                # tqdm ile ilerleme Ã§ubuÄŸu
                futures = [executor.submit(generate_question_for_chunk, (i, chunk)) 
                          for i, chunk in enumerate(chunks)]
                
                for future in tqdm(as_completed(futures), total=len(chunks), desc="Sorular oluÅŸturuluyor"):
                    data.append(future.result())
        else:
            # Seri iÅŸleme
            for i, chunk in enumerate(tqdm(chunks, desc="Sorular oluÅŸturuluyor")):
                data.append(generate_question_for_chunk((i, chunk)))
        
        # DataFrame oluÅŸtur
        df = pd.DataFrame(data)
        
        # CSV dosya adÄ± oluÅŸtur
        base_name = os.path.splitext(os.path.basename(pdf_name))[0]
        model_suffix = question_generator.model.replace("-", "_")
        csv_path = os.path.join(self.output_dir, f"{base_name}_{model_suffix}_dataset.csv")
        
        # CSV olarak kaydet
        df.to_csv(csv_path, index=False)
        
        logger.info(f"CSV veri seti oluÅŸturma tamamlandÄ±. Dosya: {csv_path}")
        return csv_path


class PDFToCSV:
    """PDF'den CSV dÃ¶nÃ¼ÅŸÃ¼m iÅŸlemi iÃ§in ana sÄ±nÄ±f."""
    
    def __init__(self, input_dir: str = "input", output_dir: str = "output",
                model: str = DEFAULT_MODEL, splitter_type: str = DEFAULT_TEXT_SPLITTER,
                chunk_size: int = 2000, chunk_overlap: int = 200,
                language: str = DEFAULT_LANGUAGE, system_prompt: Optional[str] = None,
                custom_separators: Optional[List[str]] = None,
                max_workers: int = 1):
        """
        PDF'den CSV dÃ¶nÃ¼ÅŸÃ¼m iÅŸlemini baÅŸlat.
        
        Args:
            input_dir: PDF dosyalarÄ± iÃ§in giriÅŸ dizini
            output_dir: CSV dosyalarÄ± iÃ§in Ã§Ä±ktÄ± dizini
            model: KullanÄ±lacak OpenAI modeli
            splitter_type: Metin bÃ¶lÃ¼cÃ¼ algoritmasÄ±
            chunk_size: Metin parÃ§alarÄ±nÄ±n boyutu
            chunk_overlap: ParÃ§alar arasÄ±ndaki Ã¶rtÃ¼ÅŸme
            language: Belgenin dili
            system_prompt: Sistem sÃ¼tunu iÃ§in mesaj (isteÄŸe baÄŸlÄ±)
            custom_separators: Ã–zel ayÄ±rÄ±cÄ±lar listesi (isteÄŸe baÄŸlÄ±)
            max_workers: Paralel iÅŸleme iÃ§in maksimum iÅŸ parÃ§acÄ±ÄŸÄ± sayÄ±sÄ±
        """
        self.input_dir = input_dir
        self.output_dir = output_dir
        self.model = model
        self.splitter_type = splitter_type
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.language = language
        self.system_prompt = system_prompt
        self.custom_separators = custom_separators
        self.max_workers = max_workers
        
        # BileÅŸenleri baÅŸlat
        self.pdf_processor = PDFProcessor(input_dir, language)
        self.question_generator = QuestionGenerator(model, system_prompt)
        self.dataset_creator = DatasetCreator(output_dir)
    
    def process_pdf(self, specific_file: Optional[str] = None) -> Tuple[str, Dict[str, Any], int, str]:
        """
        Bir PDF dosyasÄ±nÄ± iÅŸle ve bir CSV veri seti oluÅŸtur.
        
        Args:
            specific_file: Ä°ÅŸlenecek belirli bir PDF dosyasÄ± (isteÄŸe baÄŸlÄ±)
            
        Returns:
            Tuple[str, Dict[str, Any], int, str]: 
                - OluÅŸturulan CSV dosyasÄ±nÄ±n yolu
                - Ä°statistikler
                - ParÃ§a sayÄ±sÄ±
                - AlgÄ±lanan dil
            
        Raises:
            Exception: Ä°ÅŸlem sÄ±rasÄ±nda hata oluÅŸursa
        """
        # PDF yolunu al
        pdf_path = self.pdf_processor.get_pdf_path(specific_file)
        pdf_name = os.path.basename(pdf_path)
        
        # PDF'den metin Ã§Ä±kar
        text, stats = self.pdf_processor.extract_text_from_pdf(pdf_path)
        
        # Dili algÄ±la (auto olarak ayarlanmÄ±ÅŸsa)
        detected_language = self.pdf_processor.detect_language(text)
        
        # Metin iÅŸleyiciyi algÄ±lanan dille baÅŸlat
        text_processor = TextProcessor(detected_language)
        
        # Metni Ã¶n iÅŸle
        if SPACY_AVAILABLE and text_processor.nlp is not None:
            try:
                processed_text = text_processor.preprocess_with_spacy(text)
                logger.info("Metin spaCy ile iÅŸlendi")
            except Exception as e:
                logger.warning(f"spaCy ile iÅŸleme hatasÄ±: {str(e)}. GeliÅŸmiÅŸ metin iÅŸleme kullanÄ±lÄ±yor.")
                processed_text = text_processor.preprocess_text_advanced(text)
        else:
            processed_text = text_processor.preprocess_text_advanced(text)
        
        # Metin bÃ¶lÃ¼cÃ¼yÃ¼ baÅŸlat
        text_chunker = TextChunker(
            self.splitter_type, 
            self.chunk_size, 
            self.chunk_overlap,
            self.custom_separators
        )
        
        # Metni parÃ§alara bÃ¶l
        chunks = text_chunker.chunk_text_semantic(processed_text)
        
        # CSV veri seti oluÅŸtur
        csv_path = self.dataset_creator.create_csv_dataset(
            chunks, 
            self.question_generator, 
            pdf_name, 
            detected_language,
            self.system_prompt,
            self.max_workers
        )
        
        # Ä°statistikleri ve sonuÃ§larÄ± dÃ¶ndÃ¼r
        return csv_path, stats, len(chunks), detected_language
    
    def process_batch(self, max_files: Optional[int] = None) -> List[Tuple[str, str, int, str]]:
        """
        GiriÅŸ dizinindeki birden Ã§ok PDF dosyasÄ±nÄ± iÅŸle.
        
        Args:
            max_files: Ä°ÅŸlenecek maksimum dosya sayÄ±sÄ± (isteÄŸe baÄŸlÄ±)
            
        Returns:
            List[Tuple[str, str, int, str]]: Her dosya iÃ§in sonuÃ§lar listesi
                - PDF dosya adÄ±
                - CSV dosya yolu
                - ParÃ§a sayÄ±sÄ±
                - AlgÄ±lanan dil
        """
        # GiriÅŸ dizinindeki tÃ¼m PDF dosyalarÄ±nÄ± bul
        pdf_files = [f for f in os.listdir(self.input_dir) if f.lower().endswith('.pdf')]
        
        if max_files:
            pdf_files = pdf_files[:max_files]
        
        results = []
        
        for pdf_file in tqdm(pdf_files, desc="PDF dosyalarÄ± iÅŸleniyor"):
            try:
                logger.info(f"Ä°ÅŸleniyor: {pdf_file}")
                csv_path, stats, chunk_count, detected_language = self.process_pdf(pdf_file)
                results.append((pdf_file, csv_path, chunk_count, detected_language))
                logger.info(f"TamamlandÄ±: {pdf_file} -> {csv_path}")
            except Exception as e:
                logger.error(f"{pdf_file} dosyasÄ± iÅŸlenirken hata: {str(e)}")
        
        return results


def list_available_models():
    """KullanÄ±labilir OpenAI modellerini listele."""
    print("\nğŸ“‹ KullanÄ±labilir OpenAI Modelleri:")
    print("=" * 80)
    print(f"{'Model ID':<20} {'Model AdÄ±':<20} {'AÃ§Ä±klama'}")
    print("-" * 80)
    
    for model_id, model_info in AVAILABLE_MODELS.items():
        print(f"{model_id:<20} {model_info['name']:<20} {model_info['description']}")
    
    print("=" * 80)

def list_text_splitters():
    """KullanÄ±labilir metin bÃ¶lÃ¼cÃ¼ algoritmalarÄ±nÄ± listele."""
    print("\nğŸ“‹ KullanÄ±labilir Metin BÃ¶lÃ¼cÃ¼ AlgoritmalarÄ±:")
    print("=" * 80)
    print(f"{'BÃ¶lÃ¼cÃ¼ ID':<25} {'BÃ¶lÃ¼cÃ¼ AdÄ±':<30} {'AÃ§Ä±klama'}")
    print("-" * 80)
    
    for splitter_id, splitter_info in TEXT_SPLITTER_TYPES.items():
        print(f"{splitter_id:<25} {splitter_info['name']:<30} {splitter_info['description']}")
    
    print("=" * 80)

def list_supported_languages():
    """Desteklenen dilleri listele."""
    print("\nğŸ“‹ Desteklenen Diller:")
    print("=" * 60)
    print(f"{'Dil Kodu':<15} {'Dil AdÄ±'}")
    print("-" * 60)
    
    print(f"{'auto':<15} {'Otomatik algÄ±lama'}")
    for lang_code, lang_info in SUPPORTED_LANGUAGES.items():
        print(f"{lang_code:<15} {lang_info['name']}")
    
    print("=" * 60)

def show_version_info():
    """Versiyon bilgisini gÃ¶ster."""
    print(f"CorpusCrafter: PDF to AI Dataset Generator v{__version__}")
    print(f"Python: {sys.version.split()[0]}")
    
    # BaÄŸÄ±mlÄ±lÄ±k sÃ¼rÃ¼mlerini kontrol et
    dependencies = {
        "langchain": "langchain",
        "PyPDF2": "PyPDF2",
        "pandas": "pandas",
        "openai": "openai"
    }
    
    for dep_name, package_name in dependencies.items():
        try:
            version = importlib.metadata.version(package_name)
            print(f"{dep_name}: {version}")
        except importlib.metadata.PackageNotFoundError:
            print(f"{dep_name}: YÃ¼klÃ¼ deÄŸil")
    
    # Ä°steÄŸe baÄŸlÄ± baÄŸÄ±mlÄ±lÄ±klarÄ± kontrol et
    optional_deps = {
        "langdetect": LANGDETECT_AVAILABLE,
        "spacy": SPACY_AVAILABLE
    }
    
    print("\nÄ°steÄŸe BaÄŸlÄ± BaÄŸÄ±mlÄ±lÄ±klar:")
    for dep_name, installed in optional_deps.items():
        status = "YÃ¼klÃ¼" if installed else "YÃ¼klÃ¼ deÄŸil"
        print(f"{dep_name}: {status}")

def main():
    """Komut satÄ±rÄ± kullanÄ±mÄ± iÃ§in ana fonksiyon."""
    parser = argparse.ArgumentParser(
        description="CorpusCrafter: PDF to AI Dataset Generator - GeliÅŸmiÅŸ Metin Ä°ÅŸleme",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    parser.add_argument("--input", "-i", default="input", help="PDF dosyalarÄ± iÃ§in giriÅŸ dizini")
    parser.add_argument("--output", "-o", default="output", help="CSV dosyalarÄ± iÃ§in Ã§Ä±ktÄ± dizini")
    parser.add_argument("--file", "-f", help="Ä°ÅŸlenecek belirli bir PDF dosyasÄ± (isteÄŸe baÄŸlÄ±)")
    parser.add_argument("--model", "-m", default=DEFAULT_MODEL, 
                        help="KullanÄ±lacak OpenAI modeli")
    parser.add_argument("--splitter", "-s", default=DEFAULT_TEXT_SPLITTER,
                        help="KullanÄ±lacak metin bÃ¶lÃ¼cÃ¼ algoritmasÄ±")
    parser.add_argument("--chunk-size", "-cs", type=int, default=2000,
                        help="Metin parÃ§alarÄ±nÄ±n karakter cinsinden boyutu")
    parser.add_argument("--chunk-overlap", "-co", type=int, default=200,
                        help="ParÃ§alar arasÄ±ndaki karakter cinsinden Ã¶rtÃ¼ÅŸme")
    parser.add_argument("--language", "-l", default=DEFAULT_LANGUAGE,
                        help="Belgenin birincil dili (otomatik algÄ±lama iÃ§in 'auto')")
    parser.add_argument("--system-prompt", "-sp", 
                        help="Sistem sÃ¼tunu iÃ§in Ã¶zel mesaj")
    parser.add_argument("--batch", "-b", action="store_true",
                        help="GiriÅŸ dizinindeki tÃ¼m PDF dosyalarÄ±nÄ± iÅŸle")
    parser.add_argument("--max-files", "-mf", type=int,
                        help="Batch modunda iÅŸlenecek maksimum dosya sayÄ±sÄ±")
    parser.add_argument("--custom-separators", "-sep", nargs="+",
                        help="Metin bÃ¶lme iÃ§in Ã¶zel ayÄ±rÄ±cÄ±lar")
    parser.add_argument("--max-workers", "-w", type=int, default=1,
                        help="Paralel iÅŸleme iÃ§in maksimum iÅŸ parÃ§acÄ±ÄŸÄ± sayÄ±sÄ±")
    parser.add_argument("--verbose", "-v", action="store_true",
                        help="AyrÄ±ntÄ±lÄ± gÃ¼nlÃ¼k kaydÄ±nÄ± etkinleÅŸtir")
    parser.add_argument("--log-file", 
                        help="GÃ¼nlÃ¼k dosyasÄ±nÄ±n adÄ±")
    parser.add_argument("--test-mode", "-t", action="store_true",
                        help="Test modunu etkinleÅŸtir (OpenAI API Ã§aÄŸrÄ±larÄ± yapÄ±lmaz)")
    
    # Bilgi komutlarÄ±
    parser.add_argument("--list-models", "-lm", action="store_true",
                        help="KullanÄ±labilir OpenAI modellerini listele")
    parser.add_argument("--list-splitters", "-ls", action="store_true",
                        help="KullanÄ±labilir metin bÃ¶lÃ¼cÃ¼ algoritmalarÄ±nÄ± listele")
    parser.add_argument("--list-languages", "-ll", action="store_true",
                        help="Desteklenen dilleri listele")
    parser.add_argument("--version", action="store_true",
                        help="Versiyon bilgisini gÃ¶ster")
    
    args = parser.parse_args()
    
    # Bilgi komutlarÄ±nÄ± iÅŸle
    if args.list_models:
        list_available_models()
        return
    
    if args.list_splitters:
        list_text_splitters()
        return
    
    if args.list_languages:
        list_supported_languages()
        return
    
    if args.version:
        show_version_info()
        return
    
    # GÃ¼nlÃ¼k seviyesini ayarla
    if args.verbose:
        logger = configure_logging(
            log_file=args.log_file or "pdf_to_csv.log",
            log_level=logging.DEBUG
        )
    
    # Test modunu ayarla
    if args.test_mode:
        global TEST_MODE
        TEST_MODE = True
        os.environ["CORPUSCRAFTER_TEST_MODE"] = "true"
        logger.info("Test modu etkinleÅŸtirildi")
    
    # PDF'den CSV dÃ¶nÃ¼ÅŸÃ¼m iÅŸlemini baÅŸlat
    pdf_to_csv = PDFToCSV(
        input_dir=args.input,
        output_dir=args.output,
        model=args.model,
        splitter_type=args.splitter,
        chunk_size=args.chunk_size,
        chunk_overlap=args.chunk_overlap,
        language=args.language,
        system_prompt=args.system_prompt,
        custom_separators=args.custom_separators,
        max_workers=args.max_workers
    )
    
    try:
        if args.batch:
            # Batch iÅŸleme
            results = pdf_to_csv.process_batch(args.max_files)
            print(f"\nâœ… {len(results)} PDF dosyasÄ± baÅŸarÄ±yla iÅŸlendi.")
            
            # SonuÃ§larÄ± gÃ¶ster
            if results:
                print("\nÄ°ÅŸleme SonuÃ§larÄ±:")
                print("-" * 80)
                print(f"{'PDF DosyasÄ±':<30} {'CSV DosyasÄ±':<30} {'ParÃ§a SayÄ±sÄ±':<15} {'Dil'}")
                print("-" * 80)
                
                for pdf_file, csv_path, chunk_count, language in results:
                    csv_name = os.path.basename(csv_path)
                    print(f"{pdf_file:<30} {csv_name:<30} {chunk_count:<15} {language}")
        else:
            # Tek dosya iÅŸleme
            csv_path, stats, chunk_count, detected_language = pdf_to_csv.process_pdf(args.file)
            
            print(f"\nâœ… PDF baÅŸarÄ±yla iÅŸlendi!")
            print(f"ğŸ“Š Ä°statistikler:")
            print(f"  - Toplam sayfa: {stats['total_pages']}")
            print(f"  - Atlanan sayfa: {stats['skipped_pages']}")
            print(f"  - BoÅŸ sayfa: {stats['empty_pages']}")
            print(f"  - GÃ¶rÃ¼ntÃ¼ sayfasÄ±: {stats['image_pages']}")
            print(f"  - OluÅŸturulan parÃ§a: {chunk_count}")
            print(f"  - AlgÄ±lanan dil: {detected_language}")
            print(f"  - CSV dosyasÄ±: {csv_path}")
    
    except Exception as e:
        logger.error(f"Ä°ÅŸlem hatasÄ±: {str(e)}")
        print(f"\nâŒ Hata: {str(e)}")
        sys.exit(1)

if __name__ == "__main__":
    main()
